{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from in2110.oblig1b import visualize_word_vectors\n",
    "from in2110.corpora import aviskorpus_10_nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BIO_sequence(spans, sentence_length):\n",
    "    \"\"\"Gitt en liste over \"spans\", representert som tuples (start, end, tag),\n",
    "    og en setningslengde, produserer en sekvens med BIO (også kalt IOB) labeller\n",
    "    for setningen. \n",
    "    Eksempel: hvis spans=[(1,3,'ORG')] og sentence_length=6 bør resultatet være\n",
    "    ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O']\"\"\"\n",
    "    \n",
    "    res = ['O'] * sentence_length\n",
    "    for span in spans:\n",
    "        res[span[0]] = 'B-' + span[2]\n",
    "        for n in range(span[0]+1, span[1]):\n",
    "            res[n] = 'I-' + span[2]\n",
    "    return res\n",
    "                    \n",
    "\n",
    "def get_spans(label_sequence):\n",
    "    \"\"\"Gitt en labelsekvens med BIO markering, returner en lister over \"spans\" med \n",
    "    navngitte enheter. Metoden er altså den motsatte av get_BIO_sequence\"\"\"\n",
    "    \n",
    "    spans = []           \n",
    "    i = 0\n",
    "    while i < len(label_sequence):\n",
    "        label = label_sequence[i]\n",
    "        if label.startswith(\"B-\"):\n",
    "            start = i\n",
    "            label = label[2:]\n",
    "            end = start + 1\n",
    "            while end < len(label_sequence) and label_sequence[end].startswith(\"I-%s\"%label):\n",
    "                end += 1\n",
    "            spans.append((start, end, label))\n",
    "            i = end\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans\n",
    "\n",
    "\n",
    "def preprocess(tagged_text):\n",
    "    \"\"\"Tar en tokenisert tekst med XML tags (som f.eks. <ORG>Stortinget</ORG>) og\n",
    "    returnerer en liste over setninger (som selv er lister over tokens), sammen med\n",
    "    en liste av samme lengde som inneholder de markerte navngitte enhetene. \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    spans = []\n",
    "    \n",
    "    for i, line in enumerate(tagged_text.split(\"\\n\")):\n",
    "\n",
    "        tokens = []\n",
    "        spans_in_sentence = []\n",
    "        \n",
    "        for j, token in enumerate(line.split(\" \")):\n",
    "            \n",
    "            # Hvis token starter med en XML tag\n",
    "            start_match = re.match(\"<(\\w+?)>\", token)\n",
    "            if start_match:\n",
    "                new_span = (j, None, start_match.group(1))\n",
    "                spans_in_sentence.append(new_span)\n",
    "                token = token[start_match.end(0):]\n",
    "            \n",
    "            # Hvis token slutter med en XML tag\n",
    "            end_match = re.match(\"(.+)</(\\w+?)>$\", token)\n",
    "            if end_match:\n",
    "                if not spans_in_sentence or spans_in_sentence[-1][1]!=None:\n",
    "                    raise RuntimeError(\"Closing tag without corresponding open tag\")\n",
    "                start, _ , tag = spans_in_sentence[-1]\n",
    "                if tag != end_match.group(2):\n",
    "                    raise RuntimeError(\"Closing tag does not correspond to open tag\")\n",
    "                token = token[:end_match.end(1)]\n",
    "                spans_in_sentence[-1] = (start, j+1, tag)\n",
    "                \n",
    "            tokens.append(token)\n",
    "            \n",
    "        sentences.append(tokens)\n",
    "        spans.append(spans_in_sentence)\n",
    "        \n",
    "    return sentences, spans\n",
    "\n",
    "\n",
    "def postprocess(sentences, spans):\n",
    "    \"\"\"Gitt en liste over setninger og en tilsvarende liste over \"spans\" med\n",
    "    navngitte enheter, produserer en tekst med XML markering.\"\"\"\n",
    "    \n",
    "    tagged_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sentence = list(sentence)\n",
    "        for start, end, tag in spans[i]:\n",
    "            new_sentence[start] = \"<%s>%s\"%(tag, new_sentence[start])\n",
    "            new_sentence[end-1] = \"%s</%s>\"%(new_sentence[end-1], tag)\n",
    "        tagged_sentences.append(\" \".join(new_sentence))\n",
    "     \n",
    "    return \"\\n\".join(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntityRecogniser:\n",
    "    \"\"\"Gjenkjenning av navngitte enheter ved bruk av HMM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Intialiserer alle variablene som er nødvendig for å representere og \n",
    "        estimere  sekvensmodellen (en Hidden Markov Model) som brukes til å \n",
    "        gjenkjenne de navngitte enhetene\"\"\"\n",
    "        \n",
    "        # alle labellene som forekommer i treningsettet\n",
    "        self.labels = set()\n",
    "\n",
    "        # alle token som forekommer i treningsettet\n",
    "        self.vocab = set()\n",
    "\n",
    "        # hvor mange ganger en label (f.eks. B-ORG) forekommer i treningsettet\n",
    "        self.label_counts = {}\n",
    "\n",
    "        # hvor mange transisjoner fra label_1 til label2 forekommer i treningsettet\n",
    "        self.transition_counts = {'START':{}}\n",
    "        \n",
    "        # hvor mange emisjoner fra label til token forekommer i treningsettet\n",
    "        # (Merk at vi legger et spesielt symbol for ord som aldri forekommer\n",
    "        # i treningsettet, men kan forekomme i testsettet)\n",
    "        self.emission_counts = {(\"O\", \"<UNK>\"):1}\n",
    "                \n",
    "        # Sansynnlighet P(label_2 | label_1)\n",
    "        self.transition_probs = {}\n",
    "        \n",
    "        # Sansynnlighet P(token | label)\n",
    "        self.emission_probs = {}\n",
    "    \n",
    "    \n",
    "    def fit(self, tagged_text):\n",
    "        \"\"\"Estimerer tallene og sansynnlighetene for HMM, basert på (tokenisert)\n",
    "        tekst hvor navngitte enhetene er markert med XML tags (se norne.txt)\"\"\"\n",
    "        \n",
    "        # Ekstrahere setninger og navngitte enheter markert i hver setning\n",
    "        sentences, all_spans = preprocess(tagged_text)\n",
    "        \n",
    "        for sentence, spans in zip(sentences, all_spans):\n",
    "            \n",
    "            # Ekstrahere labelsekvenser, med BIO (også kalt IOB) marking\n",
    "            label_sequence = get_BIO_sequence(spans, len(sentence))\n",
    "\n",
    "            # Oppdatere tallene\n",
    "            self._add_counts(sentence, label_sequence)\n",
    "        \n",
    "        # Beregne sansynnlighetene (transition og emission) ut fra tallene\n",
    "        self._fill_probs()\n",
    "                   \n",
    "        \n",
    "    def _add_counts(self, sentence, label_sequence):\n",
    "        \"\"\"Oppdaterer variablene self.vocab, self.labels, self.label_counts, \n",
    "        self.transition_counts og  self.emission_counts, basert på setningen og \n",
    "        sekvenslabellen assosiert med dem. \n",
    "        Merk at setningen og label_sequence har samme lengde.\"\"\"\n",
    "        \n",
    "        # alle labellene som forekommer i treningsettet\n",
    "        self.labels |= set(label_sequence)\n",
    "\n",
    "        # alle token som forekommer i treningsettet\n",
    "        self.vocab |= set(sentence)\n",
    "\n",
    "        # hvor mange ganger en label (f.eks. B-ORG) forekommer i treningsettet\n",
    "        for label in label_sequence + ['START']:\n",
    "            if label in self.label_counts:\n",
    "                self.label_counts[label] += 1\n",
    "            else:\n",
    "                self.label_counts[label] = 1\n",
    "\n",
    "        # hvor mange transisjoner fra label_1 til label_2 forekommer i treningsettet\n",
    "        cur_label = 'START'\n",
    "        for label in label_sequence:\n",
    "            if cur_label in self.transition_counts:\n",
    "                if label in self.transition_counts[cur_label]:\n",
    "                    self.transition_counts[cur_label][label] += 1\n",
    "                else:\n",
    "                    self.transition_counts[cur_label][label] = 1\n",
    "            else:\n",
    "                self.transition_counts[cur_label] = {label:1}\n",
    "            cur_label = label\n",
    "        \n",
    "        # hvor mange emisjoner fra label til token forekommer i treningsettet\n",
    "        # (Merk at vi legger et spesielt symbol for ord som aldri forekommer\n",
    "        # i treningsettet, men kan forekomme i testsettet)\n",
    "        for label, word in zip(label_sequence, sentence):\n",
    "            if (label, word) in self.emission_counts:\n",
    "                self.emission_counts[(label, word)] += 1\n",
    "            else:\n",
    "                self.emission_counts[(label, word)] = 1\n",
    "        \n",
    "    def _fill_probs(self, alpha_smoothing=1E-6):\n",
    "        \"\"\"Beregne sannsynlihetsfordelinger self.transition_probs og\n",
    "        self.emission_probs basert på tallene som er samlet inn i \n",
    "        self.label_counts, self.transition_counts og self.emission_counts.\n",
    "        \n",
    "        Når det gjeler self.emission_probs bør vi legge Laplace smoothing, med en\n",
    "        verdi for alpha som er alpha_smoothing.\"\"\"\n",
    "        \n",
    "        # Sansynnlighet P(label_2 | label_1)\n",
    "        for label_1 in self.labels.union({'START'}):\n",
    "            self.transition_probs[label_1] = {}\n",
    "            for label_2 in self.labels:\n",
    "                # P(label_2 | label_1) = antall label_1 -> label_2 / antall label_1 -> X\n",
    "                if label_2 in self.transition_counts[label_1]:\n",
    "                    self.transition_probs[label_1][label_2] = (self.transition_counts[label_1][label_2]\n",
    "                                                                / self.label_counts[label_1])\n",
    "                else:\n",
    "                    self.transition_probs[label_1][label_2] = 0\n",
    "        \n",
    "        # Sansynnlighet P(token | label)\n",
    "        self.emission_probs = {}\n",
    "        for label in self.labels:\n",
    "            self.emission_probs[label] = {}\n",
    "            for token in self.vocab:\n",
    "                # P(token | label) = (C(label, token) +  α) / (C(label) +  α*V)\n",
    "                if (label, token) in self.emission_counts:\n",
    "                    self.emission_probs[label][token] = ((self.emission_counts[(label, token)] \n",
    "                                                        + alpha_smoothing)\n",
    "                                                        / (self.label_counts[label] \n",
    "                                                        + (alpha_smoothing * len(self.vocab))))\n",
    "                else:\n",
    "                    self.emission_probs[label][token] = (alpha_smoothing \n",
    "                                                        / (alpha_smoothing * len(self.vocab)))\n",
    "    \n",
    "    def _viterbi(self, sentence):\n",
    "        \"\"\"Kjører Viterbi-algoritmen på setningen (liste over tokens), og\n",
    "        returnerer to outputs: \n",
    "        1) en labelsekvens (som har samme lengde som setningen)\n",
    "        2) sansynnlighet for hele sekvensen \"\"\"\n",
    "\n",
    "        # De 2 datastrukturer fra Viterbi algoritmen, som dere må fylle ut\n",
    "        lattice = [{label:None for label in self.labels}\n",
    "                        for _ in range(len(sentence))]\n",
    "        backpointers = [{label:None for label in self.labels}\n",
    "                        for _ in range(len(sentence))]\n",
    "\n",
    "        # Fylle ut lattice og backpointers for setningen\n",
    "        for i, token in enumerate(sentence):\n",
    "            for label in self.labels:\n",
    "                if i == 0:\n",
    "                    lattice[0][label] = self.emission_probs[label][token]\n",
    "                else:\n",
    "                    lattice[i][label] = max([lattice[i-1][l] \n",
    "                                             * self.transition_probs[l][label] \n",
    "                                             * self.emission_probs[label][token] \n",
    "                                             for l in self.labels])\n",
    "                    backpointers[i][label] = max(lattice[i-1].items(), key=lambda x:x[1])[0]\n",
    "\n",
    "        # Finne ut det mest sannsynlig merkelapp for det siste ordet\n",
    "        best_final_label = max(lattice[-1].keys(), key=lambda x: lattice[-1][x])\n",
    "        best_final_prob = lattice[-1][best_final_label]\n",
    "\n",
    "        # Ekstrahere hele sekvensen ved å følge de \"backpointers\"\n",
    "        best_path = [best_final_label]\n",
    "        for i in range(i,0,-1):\n",
    "            best_path.insert(0, backpointers[i][best_path[0]])\n",
    "\n",
    "        # Returnerer den mest sannsynlige sekvensen (og dets sannsynlighet)\n",
    "        return best_path, best_final_prob\n",
    "    \n",
    "    def label(self, text):\n",
    "        \"\"\"Gitt en tokenisert tekst, finner ut navngitte enheter og markere disse\n",
    "        med XML tags. \"\"\"\n",
    "        sentences, _ = preprocess(text)\n",
    "        spans = []\n",
    "        for sentence in sentences:\n",
    "            sentence = [token if token in self.vocab else \"<UNK>\" for token in sentence]\n",
    "            label_sequence, _ = self._viterbi(sentence)\n",
    "            spans.append(get_spans(label_sequence))\n",
    "        \n",
    "        return postprocess(sentences, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NamedEntityRecogniser()\n",
    "ner.fit(open('norne_train.txt', 'r').read())\n",
    "ner._fill_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PER>Kjell Magne Bondevik</PER> var statsminister i <GPE>Norge</GPE> .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.label(\"Kjell Magne Bondevik var statsminister i Norge .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
