{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN3050/IN4050 Mandatory Assignment 2: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUTHOR**\n",
    "\n",
    "Stian Carlsen Swärd\n",
    "\n",
    "stiancsw@student.matnat.uio.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "Before you begin the exercise, review the rules at this website: https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html (This is an individual assignment. You are not allowed to deliver together or copy/share source-code/answers with others.)\n",
    "### Delivering\n",
    "\n",
    "**Deadline**: Wednesday, March 25, 2020, 23:59\n",
    "\n",
    "Your submission should be delivered in Devilry. You may redeliver in Devilry before the deadline, but include all files in the last delivery, as only the last delivery will be read. You are recommended to upload preliminary versions hours (or days) before the final deadline.\n",
    "\n",
    "### What to deliver?\n",
    "\n",
    "You are recommended to solve the exercise in a Jupyter notebook, but you might solve it in a Python program if you prefer.\n",
    "\n",
    "If you choose Jupyter, you should deliver the notebook. You should answer all questions and explain what you are doing in Markdown. Still, the code should be properly commented. The notebook should contain results of your runs. In addition, you should make a pdf of your solution. (If you have problems making a pdf at your own machine, you can make it at the IFI linux cluster.)\n",
    "\n",
    "If you prefer not to use notebooks, you should deliver the code, your run results, and a pdf-report where you answer all the questions and explain your work.\n",
    "\n",
    "Your report/notebook should contain your name and username.\n",
    "\n",
    "Deliver one single zipped folder (.zip, .tgz or .tar.gz) which contains your complete solution.\n",
    "\n",
    "Important: if you weren’t able to finish the assignment, use the PDF report/Markdown to elaborate on what you’ve tried and what problems you encountered. Students who have made an effort and attempted all parts of the assignment will get a second chance even if they fail initially. This exercise will be graded PASS/FAIL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of the exercise\n",
    "This exercise has three parts. The goal of the first part is to get some more experience with supervised classification. We will use simple synthetic datasets and focus on the learning algorithms. \n",
    "\n",
    "The goal of the second part is to consider the implementaion of the  Multi-layer feed forward neural network, often called Multi-layer perceptron (MLP).\n",
    "\n",
    "The third part, which is the smallest one, is dedicated to evaluation.\n",
    "\n",
    "### Tools\n",
    "The aim of the exercises is to give you a look inside the learning algorithms. You may freely use code from the weekly exercises and the published solutions. You should not use ML libraries like scikit-learn or tensorflow.\n",
    "\n",
    "You may use tools like NumPy and Pandas, which are not specific ML-tools.\n",
    "\n",
    "### Beware\n",
    "This is a new assignment. There might occur typos or ambiguities. If anything is unclear, do not hesitate to ask. Also, if you think some assumptions are missing, make your\n",
    "own and explain them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import random\n",
    "from collections import Counter\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Comparing classifiers\n",
    "## Datasets\n",
    "We start by making a synthetic dataset of 1600 datapoints and three classes, with 800 individuals in one class and 400 in each of the two other classes. (See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs regarding how the data are generated.)\n",
    "\n",
    "When we are doing experiments in supervised learning, and the data are not already split into training and test sets, we should start by splitting the data. Sometimes there are natural ways to split the data, say training on data from one year and testing on data from a later year, but if that is not the case, we should shuffle the data randomly before splitting. (OK, that is not necessary with this particular synthetic data set, since it is already shuffled by default by scikit, but that will not be the case with real-world data.) We should split the data so that we keep the alignment between X and t, which may be achieved by shuffling the indices. We split into 50% for training, 25% for validation, and 25% for final testing. The set for final testing *must not be used* till the and of the assignment in part 3.\n",
    "\n",
    "We fix the seed both for data set generation and for shuffling, so that we work on the same datasets when we rerun the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, t = make_blobs(n_samples=[400,800,400], centers=[[0,0],[1,2],[2,3]], \n",
    "                  n_features=2, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1301,  293,  968,  624,  658,  574,  433,  368,  512,  353])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(X.shape[0])\n",
    "random.seed(2020)\n",
    "random.shuffle(indices)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[indices[:800],:]\n",
    "X_val = X[indices[800:1200],:]\n",
    "X_test = X[indices[1200:],:]\n",
    "t_train = t[indices[:800]]\n",
    "t_val = t[indices[800:1200]]\n",
    "t_test = t[indices[1200:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will  make a second dataset by merging the two smaller classes in (X,t) and call the new set (X, t2). This will be a binary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_train = (t_train == 1).astype('int')\n",
    "t2_val = (t_val == 1).astype('int')\n",
    "t2_test = (t_test == 1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "def show(X, y, marker='.'):\n",
    "    labels = set(y)\n",
    "    for lab in labels:\n",
    "        plt.plot(X[y == lab][:, 1], X[y == lab][:, 0],\n",
    "                 marker, label=\"class {}\".format(lab))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X_train, t2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "We see that that set (X, t2) is far from linearly separable, and we will explore how various classifiers are able to handle this. We start with linear regression. You may use the implementation from exercise set week07 or make your own. You should make one improvement. The implementation week07 runs for a set number of epochs. You provide the number of epochs with a parameter to the fit-method. However, you do not know what a reasonable number of epochs is. Add one more argument to the fit-method *diff* (with defualt value e.g. 0.001). The training should stop when the update is less than *diff*. The *diff* will save training time, but it may also be wise to not set it too small -- and not run training for too long -- to avoid overfitting.\n",
    "\n",
    "Train the classifier on (X_train, t2_train) and test for accuracy on (X_val, t2_val) for various values of *diff*. Choose what you think is optimal *diff*. Report accuracy and save it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    # Put bias in position 0\n",
    "    sh = X.shape\n",
    "    if len(sh) == 1:\n",
    "        #X is a vector\n",
    "        return np.concatenate([np.array([1]), X])\n",
    "    else:\n",
    "        # X is a matrix\n",
    "        m = sh[0]\n",
    "        bias = np.ones((m,1)) # Makes a m*1 matrix of 1-s\n",
    "        return np.concatenate([bias, X], axis  = 1)\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    sum_errors = 0.\n",
    "    for i in range(0,len(y)):\n",
    "        sum_errors += (y[i] - y_pred[i])**2\n",
    "    mean_squared_error = sum_errors/len(y)\n",
    "    return mean_squared_error\n",
    "\n",
    "# Vector form solution\n",
    "def msr2(x,y):\n",
    "    return sum((x - y)**2) /x.shape[0]\n",
    "\n",
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to all numpy classifiers --- if any\"\"\"\n",
    "    \n",
    "    def accuracy(self,X_test, y_test, **kwargs):\n",
    "        pred = self.predict(X_test, **kwargs)\n",
    "        if len(pred.shape) > 1:\n",
    "            pred = pred[:,0]\n",
    "        return sum(pred==y_test)/len(pred)\n",
    "\n",
    "class NumpyLinRegClass(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, gamma = 0.2, epochs=100, diff=0.001):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.theta = theta = np.zeros(m+1)\n",
    "        \n",
    "        prev_mse = 10\n",
    "        epoch = 0\n",
    "        \n",
    "        while True:\n",
    "            theta -= gamma / k *  X_train.T @ (X_train @ theta - t_train)\n",
    "            new_mse = mse(t_train, (X_train @ theta))\n",
    "            if abs(prev_mse - new_mse) < diff or epoch > epochs:\n",
    "                return\n",
    "            else:\n",
    "                prev_mse = new_mse\n",
    "                epoch += 1\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        score = z @ self.theta\n",
    "        return score>threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for diff =  0.01 : 0.5125\n",
      "Accuracy for diff =  0.001 : 0.545\n",
      "Accuracy for diff =  0.0001 : 0.565\n",
      "Accuracy for diff =  1e-05 : 0.5875\n",
      "Accuracy for diff =  1e-06 : 0.6\n",
      "Accuracy for diff =  1e-07 : 0.605\n",
      "Accuracy for diff =  1e-08 : 0.6075\n",
      "Accuracy for diff =  1e-09 : 0.6075\n",
      "Accuracy for diff =  1e-10 : 0.6075\n"
     ]
    }
   ],
   "source": [
    "for n in [1 / pow(10, m) for m in range(2, 11)]:\n",
    "    lrc = NumpyLinRegClass()\n",
    "    lrc.fit(X_train, t2_train, diff=n)\n",
    "    print('Accuracy for diff = ', n, ':', lrc.accuracy(X_val, t2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "From this, we can see that the classifier plateaus at `diff = 1e-08`, so we choose that. Changing `gamma` from `0.1` to `0.2` gave a small improvement to the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Do the same for logistic regression, i.e., add the *diff*, tune it, report accuracy, and store it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation taken from the solution for exercises week 7 and adjusted to work with diff \"\"\"\n",
    "from math import log\n",
    "\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def cross_entropy_loss(target, predicted):\n",
    "    \"\"\" Calculate Cross-Entropy Loss \"\"\"\n",
    "    # Solution inspired by https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "    r = 1\n",
    "    sum_score = 0.0\n",
    "    for t, p in zip(target, predicted):\n",
    "        sum_score += t * log(0.0000000001 + abs(p))\n",
    "    return -1.0 / len(target) * sum_score\n",
    "\n",
    "class NumpyLogReg(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, gamma = 0.1, epochs=1000, diff=0.01):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.theta = theta = np.zeros(m+1)\n",
    "        prev_cel = 10\n",
    "        epoch = 0\n",
    "        \n",
    "        while True:\n",
    "            theta -= gamma / k *  X_train.T @ (self.forward(X_train) - t_train)\n",
    "            new_cel = cross_entropy_loss(t_train, (X_train @ theta))\n",
    "            if prev_cel - new_cel < diff or epoch > epochs:\n",
    "                break\n",
    "            else:\n",
    "                prev_cel = new_cel\n",
    "                epoch += 1\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.theta)\n",
    "    \n",
    "    def score(self, x):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        # score = z @ self.theta\n",
    "        return (score>threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for n = 1.0 : 0.5875\n",
      "Accuracy for n = 0.1 : 0.595\n",
      "Accuracy for n = 0.01 : 0.5975\n",
      "Accuracy for n = 0.001 : 0.605\n",
      "Accuracy for n = 0.0001 : 0.605\n",
      "Accuracy for n = 1e-05 : 0.605\n",
      "Accuracy for n = 1e-06 : 0.605\n",
      "Accuracy for n = 1e-07 : 0.605\n",
      "Accuracy for n = 1e-08 : 0.605\n",
      "Accuracy for n = 1e-09 : 0.605\n",
      "Accuracy for n = 1e-10 : 0.605\n"
     ]
    }
   ],
   "source": [
    "for n in [1 / pow(10, m) for m in range(11)]:\n",
    "    lr = NumpyLogReg()\n",
    "    lr.fit(X_train, t2_train, diff=n)\n",
    "    print('Accuracy for n =', n, ':', lr.accuracy(X_val, t2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "As with the Linear Regression model, the classifier stops improving below some value for `diff`, this time the value being `0.001`. This time changing `gamma` in either directions gave no improvements to the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-nearest neighbors (*k*NN)\n",
    "We will now compare to the *k*-nearest neighbors classifier. You may use the implementation from the week05 exercise set. Beware, though, that we represented the data differently from what we do here, using Python lists instead of numpy arrays. You therefore have to either modify the representation of the data or the code a little.\n",
    "\n",
    "Train on (X_train, t2_train) and test on (X2_val, x2_val) for various values of *k*. Choose the best *k*, report accuracy and store for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation taken from exercises week 5, adjusted for our data structure \"\"\"\n",
    "\n",
    "def majority(a):\n",
    "    counts = Counter(a)\n",
    "    return counts.most_common()[0][0]\n",
    "\n",
    "def distance_L2(a, b):\n",
    "    \"L2-distance using comprehension\"\n",
    "    s = sum((x - y) ** 2 for (x,y) in zip(a,b))\n",
    "    return s ** 0.5\n",
    "\n",
    "class PyClassifier():\n",
    "    \"\"\"Common methods to all python classifiers --- if any\"\"\"\n",
    "    \n",
    "    def accuracy(self,X_test, y_test, **kwargs):\n",
    "        \"\"\"Calculate the accuracy of the classifier \n",
    "        using the predict method\"\"\"\n",
    "#         predicted = [self.predict(a, **kwargs) for a in X_test]\n",
    "#         equal = len([(p, g) for (p,g) in zip(predicted, y_test) if p==g])\n",
    "        \n",
    "        pred_targ = np.array([[self.predict(a, **kwargs), t] for a, t in zip(X_test, y_test)])\n",
    "        equal = sum([1 if p == t else 0 for p, t in pred_targ])\n",
    "        return equal / len(y_test), pred_targ\n",
    "\n",
    "class PykNNClassifier(PyClassifier):\n",
    "    \"\"\"kNN classifier using pure python representations\"\"\"\n",
    "    \n",
    "    def __init__(self, k=3, dist=distance_L2):\n",
    "        self.k = k\n",
    "        self.dist = dist\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, a):\n",
    "        X = self.X_train\n",
    "        y = self.y_train\n",
    "        distances = [(self.dist(a, b), b, c) for (b, c) in zip(X, y)]\n",
    "        distances.sort()\n",
    "        predictors = [c for (_,_,c) in distances[0: self.k]]\n",
    "        return majority(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 10 : 0.74\n",
      "Accuracy for k = 11 : 0.7375\n",
      "Accuracy for k = 12 : 0.7525\n",
      "Accuracy for k = 13 : 0.7475\n",
      "Accuracy for k = 14 : 0.7675\n",
      "Accuracy for k = 15 : 0.7525\n",
      "Accuracy for k = 16 : 0.755\n",
      "Accuracy for k = 17 : 0.75\n",
      "Accuracy for k = 18 : 0.7575\n",
      "Accuracy for k = 19 : 0.7525\n",
      "Accuracy for k = 20 : 0.7625\n"
     ]
    }
   ],
   "source": [
    "\"\"\" I assume you mean 'test on (X_val, t2_val)' as neither X2_val, nor x2_val are defined in the precode \"\"\"\n",
    "for k in range(10, 21):\n",
    "    knnC = PykNNClassifier(k=k)\n",
    "    knnC.fit(X_train, t2_train)\n",
    "    print('Accuracy for k =', k, ':', knnC.accuracy(X_val, t2_val)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "We see that we have the highest accuracy for `k = 14`, so we choose this.\n",
    "\n",
    "*The algorithm has been tested with a wider range of values for `k`, but it's been reduced for simplicity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple perceptron\n",
    "Finally, run the simple perceptron (week05) on the same set, and report and store accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyPerClassifier(PyClassifier):\n",
    "    \"\"\"Simple perceptron python classifier\"\"\"\n",
    "    \n",
    "    def fit(self, X_train, y_train, eta=1, epochs=1, bias=1):\n",
    "        \"\"\"Train the self.weights on the training data eith learning\n",
    "        rate eta, running epochs many epochs\"\"\"\n",
    "        X_train = [[bias]+list(x) for x in X_train] # Put bias in position 0      \n",
    "        self.dim = dim = len(X_train[0])\n",
    "        self.weights = weights = [0 for _ in range(dim)]\n",
    "        # Initialize all weights to 0. There are better ways!\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for x, t in zip(X_train, y_train):\n",
    "                y = int(self.forward(x)>0)\n",
    "                for i in range(dim):\n",
    "                    weights[i] += eta * (t - y) * x[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Calculate the score for the item x\"\"\"\n",
    "        score = sum([self.weights[i]*x[i] for i in range(self.dim)])\n",
    "        return score       \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the value for the item x\"\"\"\n",
    "        x = [1] + list(x)\n",
    "        score = self.forward(x)\n",
    "        return int(score > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 10 epochs, test:  0.595\n",
      "Accuracy after 11 epochs, test:  0.583\n",
      "Accuracy after 12 epochs, test:  0.565\n",
      "Accuracy after 13 epochs, test:  0.613\n",
      "Accuracy after 14 epochs, test:  0.647\n",
      "Accuracy after 15 epochs, test:  0.593\n",
      "Accuracy after 16 epochs, test:  0.583\n",
      "Accuracy after 17 epochs, test:  0.662\n",
      "Accuracy after 18 epochs, test:  0.598\n",
      "Accuracy after 19 epochs, test:  0.595\n",
      "Accuracy after 20 epochs, test:  0.647\n",
      "Accuracy after 25 epochs, test:  0.647\n",
      "Accuracy after 30 epochs, test:  0.645\n",
      "Accuracy after 50 epochs, test:  0.585\n",
      "Accuracy after 75 epochs, test:  0.652\n",
      "Accuracy after 100 epochs, test:  0.585\n",
      "Accuracy after 150 epochs, test:  0.652\n",
      "Accuracy after 200 epochs, test:  0.625\n",
      "Accuracy after 300 epochs, test:  0.598\n"
     ]
    }
   ],
   "source": [
    "for i in [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 50, 75, 100, 150, 200, 300]:\n",
    "    cl = PyPerClassifier()\n",
    "    cl.fit(X_train, t2_train, eta= 0.1, epochs = i)\n",
    "    test = cl.accuracy(X_val, t2_val)[0]\n",
    "    print(\"Accuracy after {:2} epochs, test: {:6.3f}\".format(i, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "We see that `epochs = 17` gives the greatest accuracy score for our datasets, so we choose this.\n",
    "\n",
    "*The algorithm has been tested with a wider range of values for `epoch` than <10, 20>, but we include this for illustrative purpose.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Report the accuracies for the four classifiers in a table.\n",
    "\n",
    "Write a couple of sentences where you comment on what you see. Are the results as you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The following are results from our testing of binary classifiers:\n",
    "\n",
    "| Algorithm | Optimal variable | Accuracy |\n",
    "|-----------|------------------|----------|\n",
    "| Linear Regression | `diff = 1e-08` | 0.6075 |\n",
    "| Logistic Regresstion | `diff = 0.001` | 0.605 |\n",
    "| k Nearest Neighbours | `k = 14` | 0.7675 |\n",
    "| Simple Perceptron | `epochs = 17` | 0.662 |\n",
    "\n",
    "We see that none of these classifiers are particularly good at predicting our non-linearly dividable binary dataset. The best we can do is the k-Nearest-Neighbours classifier, which manages just over 75% accuracy. Considering the distribution of our data set, this is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classifiers\n",
    "We now turn to the task of classifying when there are more than two classes, and the task is to ascribe one class to each input. We will now use the set (X, t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*NN\n",
    "One of the classifiers can handle multiple classes without modifications: the *k*-nearest neighbors classifier. Train it on (X_train, t_train), test it on (X_val, t_val) for various values of *k*. Choose the one you find best and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 10 : 0.745\n",
      "Accuracy for k = 11 : 0.7475\n",
      "Accuracy for k = 12 : 0.755\n",
      "Accuracy for k = 13 : 0.75\n",
      "Accuracy for k = 14 : 0.77\n",
      "Accuracy for k = 15 : 0.7575\n",
      "Accuracy for k = 16 : 0.76\n",
      "Accuracy for k = 17 : 0.7525\n",
      "Accuracy for k = 18 : 0.76\n",
      "Accuracy for k = 19 : 0.755\n",
      "Accuracy for k = 20 : 0.7675\n",
      "Accuracy for k = 21 : 0.76\n",
      "Accuracy for k = 22 : 0.765\n",
      "Accuracy for k = 23 : 0.7525\n",
      "Accuracy for k = 24 : 0.755\n",
      "Accuracy for k = 25 : 0.7525\n",
      "Accuracy for k = 26 : 0.7525\n",
      "Accuracy for k = 27 : 0.75\n",
      "Accuracy for k = 28 : 0.755\n",
      "Accuracy for k = 29 : 0.755\n",
      "Accuracy for k = 30 : 0.755\n"
     ]
    }
   ],
   "source": [
    "for k in range(10, 31):\n",
    "    knnC = PykNNClassifier(k=k)\n",
    "    knnC.fit(X_train, t_train)\n",
    "    print('Accuracy for k =', k, ':', knnC.accuracy(X_val, t_val)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Again, `k = 14` gives the best result, an accuracy of 77% this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression \"one-vs-rest\"\n",
    "We saw in the lecture how a logistic regression classifier can be turned into a multi-class classifier using the one-vs-rest approach. We train one classifier for each class and assign the class which ascribes the highest probability.\n",
    "\n",
    "Extend the logisitc regression classifier to a multi-class classifier. To do this, you must modify the target values from scalars to arrays. Train the resulting classifier on (X_train, t_train), test it on (X_val, t_val), and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We modify the original binary Logistic Regression class to return its confidence rather than the answer \"\"\"\n",
    "\n",
    "class MultiLogRegBase(NumpyClassifier):\n",
    "\n",
    "    def fit(self, X_train, t_train, gamma = 0.1, epochs=1000, diff=0.01):\n",
    "        \"\"\"X_train is a kxm matrix, k data points, m features\n",
    "        t_train are the target values for training data\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.theta = theta = np.zeros(m+1)\n",
    "        prev_cel = 10\n",
    "        epoch = 0\n",
    "        \n",
    "        while True:\n",
    "            theta -= gamma / k *  X_train.T @ (self.forward(X_train) - t_train)\n",
    "            new_cel = cross_entropy_loss(t_train, (X_train @ theta))\n",
    "            if prev_cel - new_cel < diff or epoch > epochs:\n",
    "                break\n",
    "            else:\n",
    "                prev_cel = new_cel\n",
    "                epoch += 1\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return logistic(X @ self.theta)\n",
    "    \n",
    "    def score(self, x):\n",
    "        z = add_bias(x)\n",
    "        score = self.forward(z)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        z = add_bias(x)\n",
    "        return z @ self.theta\n",
    "\n",
    "class MultiLogRegClass(NumpyClassifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classifiers = []\n",
    "    \n",
    "    def fit(self, X_train, t_train, gamma=0.2, epochs=1000, diff=0.1):\n",
    "        \"\"\" Create one classifier for each class in t_train,\n",
    "        train them individually using given parameters \"\"\"\n",
    "        \n",
    "        classes = set(t_train)\n",
    "        \n",
    "        for c in classes:\n",
    "            self.classifiers.append(MultiLogRegBase())\n",
    "            self.classifiers[-1].fit(X_train, [1 if t == c else 0 for t in t_train], gamma=gamma, epochs=epochs, diff=diff)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append([])\n",
    "            for c in self.classifiers:\n",
    "                predictions[-1].append(c.predict(x))\n",
    "        ret = np.array([x.index(max(x)) for x in predictions])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for diff = 5 : 0.58\n",
      "Accuracy for diff = 2 : 0.58\n",
      "Accuracy for diff = 1 : 0.58\n",
      "Accuracy for diff = 0.5 : 0.58\n",
      "Accuracy for diff = 0.2 : 0.5825\n",
      "Accuracy for diff = 0.1 : 0.5825\n",
      "Accuracy for diff = 0.05 : 0.585\n",
      "Accuracy for diff = 0.02 : 0.5875\n",
      "Accuracy for diff = 0.01 : 0.59\n",
      "Accuracy for diff = 0.005 : 0.595\n",
      "Accuracy for diff = 0.002 : 0.6025\n",
      "Accuracy for diff = 0.001 : 0.605\n",
      "Accuracy for diff = 0.0005 : 0.605\n",
      "Accuracy for diff = 0.0002 : 0.605\n",
      "Accuracy for diff = 0.0001 : 0.605\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001]:\n",
    "    mlrc = MultiLogRegClass()\n",
    "    mlrc.fit(X_train, t_train, diff=n)\n",
    "    print('Accuracy for diff =', n, ':', mlrc.accuracy(X_val, t_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results in a couple of sentences, addressing questions like\n",
    "\n",
    "- How do the two classfiers compare?\n",
    "- How do the results on the three-class classification task compare to the results on the binary task?\n",
    "- What do you think are the reasons for the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "* The kNN classifier beats the Logistic Regression classifier by a significant margin, but none of them are particularly good.\n",
    "* The results for the three-class classification task are very similar to the binary classification for the same classifiers.\n",
    "* Because the data points are distributed somewhat along a line in the two dimensions, and Class 1 occupies most of the middle area, it is expected that a one-vs-rest logistic regression model would have a strong bias toward that class, to the detriment of the others. We see that it does not predict any of the data points belonging to Class 2, and very few to Class 0, this is expected considering the distribution of the three classes. The kNN classifier is inherently much better at predicting for this kind of distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are returning to the binary classifier and the set (X, t2). As we see, some of the classifiers are not doing too well on the (X, t2) set. It is easy to see from the plot that this data set is not well suited for linear classifiers. There are several possible options for trying to learn on such a set. One is to construct new features from the original features to get better discriminants. This works e.g. on the XOR-problem. The current classifiers use two features: $x_1$ and $x_2$ (and a bias term $x_0$). Try to add three additional features of the form ${x_1}^2$, ${x_2}^2$, $x_1*x_2$ to the original features and see what the accuracies are now. Compare to the results for the original features in a 4x2 table.\n",
    "\n",
    "Explain in a couple of sentences what effect the non-linear features have on the various classifiers. (By the way, some of the classifiers could probably achieve better results if we scaled the data, but we postpone scaling to part 2 of the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a new dataset based on X and add the new features \"\"\"\n",
    "\n",
    "X_train2 = np.array([[x[0], x[1], x[0]*x[0], x[1]*x[1], x[0]*x[1]] for x in X_train])\n",
    "X_val2 = np.array([[x[0], x[1], x[0]*x[0], x[1]*x[1], x[0]*x[1]] for x in X_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for diff =  1.0 : 0.5275\n",
      "Accuracy for diff =  0.1 : 0.5275\n",
      "Accuracy for diff =  0.01 : 0.5275\n",
      "Accuracy for diff =  0.001 : 0.5275\n",
      "Accuracy for diff =  0.0001 : 0.5275\n",
      "Accuracy for diff =  1e-05 : 0.5275\n",
      "Accuracy for diff =  1e-06 : 0.5275\n",
      "Accuracy for diff =  1e-07 : 0.5275\n",
      "Accuracy for diff =  1e-08 : 0.5275\n",
      "Accuracy for diff =  1e-09 : 0.5275\n",
      "Accuracy for diff =  1e-10 : 0.5275\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test the Linear Classifier on the new dataset \"\"\"\n",
    "for diff in [1 / pow(10, m) for m in range(11)]:\n",
    "    lrc = NumpyLinRegClass()\n",
    "    lrc.fit(X_train2, t2_train, gamma=0.1, diff=diff)\n",
    "    print('Accuracy for diff = ', diff, ':', lrc.accuracy(X_val2, t2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for diff = 1.0 : 0.615\n",
      "Accuracy for diff = 0.1 : 0.6325\n",
      "Accuracy for diff = 0.01 : 0.705\n",
      "Accuracy for diff = 0.001 : 0.69\n",
      "Accuracy for diff = 0.0001 : 0.69\n",
      "Accuracy for diff = 1e-05 : 0.69\n",
      "Accuracy for diff = 1e-06 : 0.69\n",
      "Accuracy for diff = 1e-07 : 0.69\n",
      "Accuracy for diff = 1e-08 : 0.69\n",
      "Accuracy for diff = 1e-09 : 0.69\n",
      "Accuracy for diff = 1e-10 : 0.69\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Logistic Regression Classifier \"\"\"\n",
    "for diff in [1 / pow(10, m) for m in range(11)]:\n",
    "    lr = NumpyLogReg()\n",
    "    lr.fit(X_train2, t2_train, diff=diff)\n",
    "    print('Accuracy for diff =', diff, ':', lr.accuracy(X_val2, t2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for k = 10 : 0.74\n",
      "Accuracy for k = 11 : 0.7375\n",
      "Accuracy for k = 12 : 0.7525\n",
      "Accuracy for k = 13 : 0.7475\n",
      "Accuracy for k = 14 : 0.7675\n",
      "Accuracy for k = 15 : 0.7525\n",
      "Accuracy for k = 16 : 0.755\n",
      "Accuracy for k = 17 : 0.75\n",
      "Accuracy for k = 18 : 0.7575\n",
      "Accuracy for k = 19 : 0.7525\n",
      "Accuracy for k = 20 : 0.7625\n"
     ]
    }
   ],
   "source": [
    "\"\"\" k-Nearest-Neighbors Classifier \"\"\"\n",
    "for k in range(10, 21):\n",
    "    knnC = PykNNClassifier(k=k)\n",
    "    knnC.fit(X_train, t2_train)\n",
    "    print('Accuracy for k =', k, ':', knnC.accuracy(X_val, t2_val)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 10 epochs, test:  0.637\n",
      "Accuracy after 11 epochs, test:  0.645\n",
      "Accuracy after 12 epochs, test:  0.635\n",
      "Accuracy after 13 epochs, test:  0.647\n",
      "Accuracy after 14 epochs, test:  0.630\n",
      "Accuracy after 15 epochs, test:  0.635\n",
      "Accuracy after 16 epochs, test:  0.632\n",
      "Accuracy after 17 epochs, test:  0.640\n",
      "Accuracy after 18 epochs, test:  0.637\n",
      "Accuracy after 19 epochs, test:  0.640\n",
      "Accuracy after 20 epochs, test:  0.637\n",
      "Accuracy after 25 epochs, test:  0.645\n",
      "Accuracy after 30 epochs, test:  0.642\n",
      "Accuracy after 50 epochs, test:  0.635\n",
      "Accuracy after 75 epochs, test:  0.642\n",
      "Accuracy after 100 epochs, test:  0.642\n",
      "Accuracy after 150 epochs, test:  0.630\n",
      "Accuracy after 200 epochs, test:  0.640\n",
      "Accuracy after 300 epochs, test:  0.637\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple Perceptron Classifier \"\"\"\n",
    "for i in [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 50, 75, 100, 150, 200, 300]:\n",
    "    cl = PyPerClassifier()\n",
    "    cl.fit(X_train2, t2_train, eta= 0.1, epochs = i)\n",
    "    test = cl.accuracy(X_val2, t2_val)[0]\n",
    "    print(\"Accuracy after {:2} epochs, test: {:6.3f}\".format(i, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Table of best accuracy scores for each classifier for original dataset and with added non-linear features:\n",
    "\n",
    "| Classifier | Original dataset | Non-linear features |\n",
    "|---|---|---|\n",
    "| Linear Regression | 0.6075 | 0.5275 |\n",
    "| Logistic Regression | 0.605 | 0.705 |\n",
    "| k-Nearest-Neighbors | 0.7675 | 0.7675 |\n",
    "| Simple Perceptron | 0.662 | 0.647 |\n",
    "\n",
    "We see that the Linear Regression classifier performs *worse* with the added non-linear features. Whether this is a natural consequence of the added features being *non-linear* or there is something wrong with my implementation is not obvious from the testing.\n",
    "\n",
    "The Logistic Regression classifier improved somewhat. The Logistic Regression classifier should at least be able to handle non-linear features better than the Linear Regression classifier, so at least *some* improvement over it is expected.\n",
    "\n",
    "The k-Nearest-Neighbors classifier saw no changes with the added features. This is also expected, as it operates based on the relative position between the data points, which remain unchanged when adding  new features based solely on already-existing features.\n",
    "\n",
    "The Simple Perceptron saw very little change, though slightly for the worse overall. The Perceptron classifiers have a higher risk of overfitting with a higher number of features. Since our added features are based on already-existing ones, the impact is limited, but I believe we still see a higher amount of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "## Multi-layer neural networks\n",
    "We will now implement the Multi-layer feed forward network (MLP, Marsland sec. 4.2.1). We will do it in two steps. In the first step, we will work concretely with the dataset (X, t). We will initailize the network and run a first round of training, i.e. one pass throught the algorithm at p. 78 in Marsland.\n",
    "\n",
    "In the second step, we will turn this code into a more general classifier. We can train and test this on (X, t), but also on other datasets.\n",
    "\n",
    "First of all, you should scale the X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def scale(X, mins, maxs):\n",
    "    r, d = X.shape\n",
    "    ret = np.zeros((r, d))\n",
    "    for dim in range(d):\n",
    "        for row in range(r):\n",
    "            ret[row][dim] = (X[row][dim] - mins[dim]) / (maxs[dim] - mins[dim])\n",
    "    return ret\n",
    "\n",
    "def add_bias(X):\n",
    "    # Put bias in position 0\n",
    "    sh = X.shape\n",
    "    if len(sh) == 1:\n",
    "        #X is a vector\n",
    "        return np.concatenate([np.array([1]), X])\n",
    "    else:\n",
    "        # X is a matrix\n",
    "        m = sh[0]\n",
    "        bias = - 1 * np.ones((m,1)) # Makes a m*1 matrix of 1-s\n",
    "        return np.concatenate([bias, X], axis  = 1)\n",
    "\n",
    "# Take min/max values from each dimension of training set\n",
    "mins = X_train.min(axis=0)\n",
    "maxs = X_train.max(axis=0)\n",
    "\n",
    "# Create scaled training and validation sets\n",
    "X_train_scaled = scale(X_train, mins, maxs)\n",
    "X_val_scaled = scale(X_val, mins, maxs)\n",
    "\n",
    "# Convert target values from integers to vectors\n",
    "t_train_vec = [[1 if i == t else 0 for i in range(len(set(t_train)))] for t in t_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: One round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializing\n",
    "We will only use one hidden layer. The number of nodes in the hidden layer will be a hyper-parameter provided by the user; let's call it *dim_hidden*. (*dim_hidden* is called *M* by Marsland.) Initially, we will set it to 6. This is a hyper-parameter where other values may give better results, and the hyper-parameter could be tuned.\n",
    "\n",
    "Another hyper-parameter set by the user is the learning rate. We set the initial value to 0.01, but also this may need tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01 #Learning rate\n",
    "dim_hidden = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the input *X_train* (after scaling) is a matrix of dimension *P x dim_in*, where *P* is the number of training instances, and *dim_in* is the number of features in the training instances (*L* in Marsland). Hence we can read *dim_in* off from *X_train*. Similarly, we can read *dim_out* off from *y_train*. Beware that *y_train* must be given the form of *P x dim_out* at some point, cf. the \"one-vs-all\" exercise above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = len(X_train[0])\n",
    "dim_out = len(set(t_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two sets of weights: weights1 between the input and the hidden layer, and weights2, between the hidden layer and the \n",
    "\n",
    "output. Make the weight matrices and initialize them to small random numbers. Make sure that you take the bias terms into consideration and get the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "V = np.random.rand(dim_in + 1, dim_hidden)\n",
    "W = np.random.rand(dim_hidden + 1, dim_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forwards phase\n",
    "We will run the first step in the training, and start with the forward phase. Calculate the activations after the hidden layer and after the output layer. We will follow Marsland and use the logistic (sigmoid) activation function in both layers. Inspect whether the results seem reasonable with respect to format and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\"\"\" Repeat the logistic activation function \"\"\"\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def activate(inputs, weights):\n",
    "    return logistic(inputs @ weights)\n",
    "\n",
    "hid_acts = add_bias(activate(add_bias(X_train_scaled), V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "out_acts = activate(hid_acts, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "These results seem very reasonable with respect to format and values. Unless I have misunderstood, there should be one set of activations of size equal to the number of inputs for the next layer for each input datapoint, thus `hidden_activations` should have shape `(800, 7)` and `output_activations` should have shape `(800, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards phase\n",
    "Calculate the delta terms at the output. We assume, like Marsland, that we use sums of squared errors. (This amounts to the same as using the mean square error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "out_err = out_acts * (1 - out_acts) * (t_train_vec - out_acts)\n",
    "print(out_err.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the error in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 6)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "hid_err = hid_acts[:,1:] * (1 - hid_acts[:,1:]) * (out_err @ W[1:,:].T)\n",
    "print(hid_err.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights.\n",
    "Check that they have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before updating\n",
      "V:\n",
      " [[0.84276507 0.82315282 0.71784748 0.32810878 0.03669763 0.500533  ]\n",
      " [0.54961402 0.79027548 0.39902267 0.45046081 0.39132326 0.88394184]\n",
      " [0.11880656 0.3304584  0.00485357 0.12995267 0.37099723 0.25852676]]\n",
      "W:\n",
      " [[0.76445943 0.77236915 0.95393995]\n",
      " [0.81152695 0.08104003 0.08342925]\n",
      " [0.78808268 0.04953836 0.25482434]\n",
      " [0.32293121 0.15817464 0.56825474]\n",
      " [0.12959145 0.83379513 0.05746951]\n",
      " [0.23710711 0.75412461 0.32513606]\n",
      " [0.21494825 0.57490587 0.87203239]]\n",
      "\n",
      "After updating\n",
      "V:\n",
      " [[ 0.98387982  0.98873901  0.84921507  0.39706671  0.15337288  0.68508502]\n",
      " [ 0.46707303  0.69717563  0.333725    0.4150413   0.33334844  0.79757131]\n",
      " [ 0.02300478  0.22368823 -0.06511291  0.09554037  0.31197719  0.17063481]]\n",
      "W:\n",
      " [[ 1.41751346  0.96398195  1.51807053]\n",
      " [ 0.55383709  0.00985362 -0.12202672]\n",
      " [ 0.48012675 -0.03185637  0.02083409]\n",
      " [ 0.07244352  0.08679591  0.3612447 ]\n",
      " [-0.20265435  0.74075128 -0.21326697]\n",
      " [-0.16366793  0.64413323  0.00266136]\n",
      " [-0.14644091  0.47767151  0.59175334]]\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "\n",
    "print('Before updating')\n",
    "print('V:\\n', V)\n",
    "print('W:\\n', W)\n",
    "\n",
    "new_V = V + step_size * (add_bias(X_train_scaled).T @ hid_err)\n",
    "new_W = W + step_size * (hid_acts.T @ out_err)\n",
    "\n",
    "print('\\nAfter updating')\n",
    "print('V:\\n', new_V)\n",
    "print('W:\\n', new_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2: A Multi-layer neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to train and test a classifier on (X, t). You could have put some parts of the code in the last step into a loop and run it through some iterations. But instead of copying code for every network we want to train, we will build a general Multi-layer neural network classfier as a class. This class will have some of the same structure as the classifiers we made for linear and logistic regression. The task consists mainly in copying in parts from what you did in step 1 into the template below. Remember to add the *self*- prefix where needed, and be careful in your use of variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNNClassifier():\n",
    "    \"\"\"A multi-layer neural network with one hidden layer\"\"\"\n",
    "    \n",
    "    def __init__(self,eta = 0.001, dim_hidden = 6):\n",
    "        \"\"\"Intialize the hyperparameters\"\"\"\n",
    "        self.eta = eta\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        # Should you put additional code here?\n",
    "        \n",
    "    def fit(self, X_train, t_train, epochs = 100):\n",
    "        \"\"\"Intialize the weights. Train *epochs* many epochs.\"\"\"\n",
    "        \n",
    "        # Initilaization\n",
    "        self.dim_in = len(X_train[0])\n",
    "        if type(t_train[0]) is not np.ndarray:\n",
    "            # t_train is not vectorized\n",
    "            self.dim_out =  len(set(t_train))\n",
    "            t_train = [[1 if i == t else 0 for i in range(self.dim_out)] for t in t_train]\n",
    "        else:\n",
    "            self.dim_out = len(t_train[0])\n",
    "        \n",
    "        self.V = np.random.rand(self.dim_in + 1, self.dim_hidden)\n",
    "        self.W = np.random.rand(self.dim_hidden + 1, self.dim_out)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            # Feed forward and get activations\n",
    "            hid_acts, out_acts = self.forward(X_train)\n",
    "            \n",
    "            # Calculate errors\n",
    "            out_err = out_acts * (1 - out_acts) * (t_train - out_acts)\n",
    "            hid_err = hid_acts[:,1:] * (1 - hid_acts[:,1:]) * (out_err @ self.W[1:,:].T)\n",
    "\n",
    "            # Update weights\n",
    "            self.V = self.V + self.eta * (add_bias(X_train).T @ hid_err)\n",
    "            self.W = self.W + self.eta * (hid_acts.T @ out_err)\n",
    "\n",
    "    def add_bias(self, X):\n",
    "        # Put bias in position 0\n",
    "        sh = X.shape\n",
    "        if len(sh) == 1:\n",
    "            #X is a vector\n",
    "            return np.concatenate([np.array([1]), X])\n",
    "        else:\n",
    "            # X is a matrix\n",
    "            m = sh[0]\n",
    "            bias = -1 * np.ones((m,1)) # Makes a m*1 matrix of 1-s\n",
    "            return np.concatenate([bias, X], axis  = 1)\n",
    "    \n",
    "    def logistic(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def activate(self, inputs, weights):\n",
    "        return self.logistic(inputs @ weights)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform one forward step. \n",
    "        Return a pair consisting of the outputs of the hidden_layer\n",
    "        and the outputs on the final layer\"\"\"\n",
    "        hid_acts = self.add_bias(self.activate(add_bias(X), self.V))\n",
    "        return hid_acts, self.activate(hid_acts, self.W)\n",
    "\n",
    "    def accuracy(self, X_test, t_test):\n",
    "        \"\"\"Calculate the accuracy of the classifier on the pair (X_test, t_test)\n",
    "        Return the accuracy\"\"\"\n",
    "        preds = np.array([np.argmax(p) for p in self.forward(X_test)[1]])\n",
    "        correct = 0\n",
    "        for p, t in zip(preds, t_test):\n",
    "            correct += 1 if p == t else 0\n",
    "        return correct / len(preds), np.column_stack((preds, t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "*I could not figure out the proper numpy matrix operations to perform the weight updates in time for the deadline. The way it's currently done is extremely slow and puts practical limits on the hyperparameters that I'm not completely satisfied with. I would have wanted to train it using a lower learning rate and a much higher number of epochs, but it would take far too long to train with these parameters.*\n",
    "\n",
    "*I'll catch up on Numpy matrix operations before next assignment, but any feedback as to what went wrong in my implementation this time is appreciated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network on (X_train, t_train) (after scaling), and test on (X_val, t_val). Adjust hyperparameters or number of epochs if you are not content with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "mnn = MNNClassifier(eta=0.001)\n",
    "# Find min/max values of training set\n",
    "mins = X_train.min(axis=0)\n",
    "maxs = X_train.max(axis=0)\n",
    "\n",
    "mnn.fit(scale(X_train, mins, maxs), t_train, epochs=5000)\n",
    "print(mnn.accuracy(scale(X_val, mins, maxs), t_val)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Final testing\n",
    "Take the best classifiers that you found for the training sets (X, t) and (X, t2) and test them on (X_test, t_test) and (X_test, t2_test), respectively. Compute accuracy, the confusion matrix, precision and recall. Answer in 2-3 sentences: How do the accuracies compare to the results on the validation sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Multi-class classifiers ------\n",
      "--- k-Nearest-Neighbors ---\n",
      "Accuracy : 0.7475\n",
      "Confusion matrix:\n",
      "[[ 69.  10.   0.]\n",
      " [ 23. 159.  42.]\n",
      " [  0.  26.  71.]]\n",
      "\n",
      "Precision:\n",
      "Class 0 0.8734177215189873\n",
      "Class 1 0.7098214285714286\n",
      "Class 2 0.7319587628865979\n",
      "\n",
      "Recall:\n",
      "Class 0 0.75\n",
      "Class 1 0.8153846153846154\n",
      "Class 2 0.6283185840707964\n",
      "\n",
      "--- Multi-Layer Perceptron ---\n",
      "Accuracy : 0.7525\n",
      "Confusion matrix:\n",
      "[[ 70.   8.   0.]\n",
      " [ 22. 166.  48.]\n",
      " [  0.  21.  65.]]\n",
      "\n",
      "Precision:\n",
      "Class 0 0.8974358974358975\n",
      "Class 1 0.7033898305084746\n",
      "Class 2 0.7558139534883721\n",
      "\n",
      "Recall:\n",
      "Class 0 0.7608695652173914\n",
      "Class 1 0.8512820512820513\n",
      "Class 2 0.5752212389380531\n",
      "\n",
      "------ Binary Classifiers ------\n",
      "--- k-Nearest-Neighbors ---\n",
      "Accuracy : 0.75\n",
      "Confusion matrix:\n",
      "[[141.  36.]\n",
      " [ 64. 159.]]\n",
      "\n",
      "Precision:\n",
      "Class 0 0.7966101694915254\n",
      "Class 1 0.7130044843049327\n",
      "\n",
      "Recall:\n",
      "Class 0 0.6878048780487804\n",
      "Class 1 0.8153846153846154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_confusion_matrix(res, num_classes):\n",
    "    print('Confusion matrix:')\n",
    "    matrix = np.zeros((num_classes, num_classes))\n",
    "    for r in res:\n",
    "        matrix[r[0]][r[1]] += 1\n",
    "    print(matrix)\n",
    "\n",
    "def print_precision_recall(res, num_classes):\n",
    "    precision = [0.0] * num_classes\n",
    "    recall = [0.0] * num_classes\n",
    "    for c in range(num_classes):\n",
    "        true_pos = 0\n",
    "        false_neg = 0\n",
    "        false_pos = 0\n",
    "        for row in res:\n",
    "            if row[0] == c:\n",
    "                if row[1] == c:\n",
    "                    true_pos += 1\n",
    "                else:\n",
    "                    false_pos += 1\n",
    "            else:\n",
    "                if row[1] == c:\n",
    "                    false_neg += 1\n",
    "        precision[c] = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0.0\n",
    "        recall[c] = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0.0\n",
    "    print('\\nPrecision:')\n",
    "    for c in range(num_classes):\n",
    "        print('Class', c, precision[c])\n",
    "    print('\\nRecall:')\n",
    "    for c in range(num_classes):\n",
    "        print('Class', c, recall[c])\n",
    "    print()\n",
    "\n",
    "print('------ Multi-class classifiers ------')\n",
    "\n",
    "num_classes = len(set(t_test))\n",
    "\n",
    "print('--- k-Nearest-Neighbors ---')\n",
    "knn = PykNNClassifier(k=14)\n",
    "knn.fit(X_train, t_train)\n",
    "knn_accuracy, knn_res = knn.accuracy(X_test, t_test)\n",
    "print('Accuracy :', knn_accuracy)\n",
    "print_confusion_matrix(knn_res, num_classes)\n",
    "print_precision_recall(knn_res, num_classes)\n",
    "\n",
    "print('--- Multi-Layer Perceptron ---')\n",
    "mlp = MNNClassifier(eta=0.001)\n",
    "mins = X_train.min(axis=0)\n",
    "maxs = X_train.max(axis=0)\n",
    "mnn.fit(scale(X_train, mins, maxs), t_train, epochs=5000)\n",
    "mnn_accuracy, mnn_res = mnn.accuracy(scale(X_test, mins, maxs), t_test)\n",
    "print('Accuracy :', mnn_accuracy)\n",
    "print_confusion_matrix(mnn_res, num_classes)\n",
    "print_precision_recall(mnn_res, num_classes)\n",
    "\n",
    "print('------ Binary Classifiers ------')\n",
    "print('--- k-Nearest-Neighbors ---')\n",
    "knn = PykNNClassifier(k=14)\n",
    "knn.fit(X_train, t2_train)\n",
    "knn_accuracy, knn_res = knn.accuracy(X_test, t2_test)\n",
    "print('Accuracy :', knn_accuracy)\n",
    "print_confusion_matrix(knn_res, 2)\n",
    "print_precision_recall(knn_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "*I have included two classifiers for the multi-class part. This is because I suspect a proper implementation of the MLP classifier would perform better than the kNN classifier, but since I do not have a proper implementation of it I also included the classifier that actually performed best.*\n",
    "\n",
    "*In terms of the classifiers' performance on the test set vs on the validation set, they are very similar. This is because none of the classifiers actually do anything that could cause them to fit their learning to the validation set.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
